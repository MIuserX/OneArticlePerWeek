# 原文

https://dzone.com/articles/postgresql-why-and-how-wal-bloats

# 翻译

## WAL. 一个简单介绍

对 PG 数据库的任何修改，首先被存储到 WAL，这样数据永远不会丢失。在这之后，内存页中（所谓的 "buffer cache"）的数据才会被修改并被标记为 “脏页” - 意味着它们需要被同步到磁盘。

存在一个 Checkpoint 进程，周期性地运行，它将所有的 “脏” 页写入磁盘。它也将位置保存到 WAL（叫做 **REDO point**），从开始到这个位置的数据库修改都已被同步到磁盘。

在 PG crash 时，它将会从 **REDO point** 开始顺序 replay WAL记录，来恢复状态。这样，恢复不再需要在这个点之前的 WAL 记录，但仍可能被主从复制需要 或者 被 PITR(Point In Time Recovery) 需要。

从这段描述中，一个超级工程师可能已经找出了，在实际生活中所有可能犯错的方式 :-)。但在现实生活中，一个人通常将以一种 reactive way：一个人首先需要在一个问题上跌倒。



## WAL 膨胀 #1

我们在每个 PG 实例上的监控工具将会发现 WAL 文件并收集它们的数量和总大小。

下面是一些奇怪的情况，WAL 总大小和 segment 数量增到大到了6倍：

>图

原因可能是什么呢？

做一次 checkpoint 之后 WAL 被认为不再被需要并被删除。这是我们为什么首先检查它的原因。PG 有个特别的系统视图叫做 `pg_stat_bgwriter` ，包含了一些关于 checkpoint 的信息：

* **checkpoints_timed** - 由于距离前一个 checkpoint 
* **checkpoints_req** - 



## WAL 膨胀 #2

能让你恢复到过去的任何时间点的备份称得上是好备份。

如果 “某人”（当然，不是你！）在主库上执行了下面的命令：

```sql
DELETE FROM very_important_tbl;
```

你最好有方法将数据库恢复到刚好在这个事务之前。这叫做 PITR (Point-In-Time-Recovery)。

在 PG 中，你需要周期性地做 全量备份，并启用 WAL 归档。对于这个，有个特殊的配置参数 - `archive_command` 和一个特别的进程 `postgres: archiver process` 。周期性地按你的选择运行这个命令，如果这个命令没有返回错误，就会删除相应的 WAL segment 文件。如果归档时发生了错误，这在广泛的使用云架构时变得普遍（是的，我在看着你，AWS S3），PG 将会不断重试，直到成功。这会导致磁盘上堆积大量的 WAL segments 文件，并逐渐耗尽磁盘空间。

下面是一个时间短的 WAL 归档的统计图：

> 图

